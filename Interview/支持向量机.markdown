# 支持向量机
## 《统计学习方法》上的知识点
1.支持向量机是一种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，通过引入核技巧可以使其成为非线性分类器。  
2.SVM的学习策略：间隔最大化  
3.SVM的分类：线性可分支持向量机、线性支持向量机、非线性支持向量机    
    训练数据线性可分      -->  硬间隔最大化   -->    线性分类器  
    训练数据近似线性可分  -->  软间隔最大化   -->    线性分类器  
    训练数据线性不可分    -->  核技巧+软间隔最大化  -->   非线性分类器  
4.线性可分支持向量机  
    
5.线性支持向量机
    针对的问题：数据集中有一些离群点线性不可分，但大部分样本点是线性可分的
    解决方法：在约束条件上加一个松弛变量，表示有样本点到超平面的距离小于函数间隔的超平面也接受；同时在损失函数上加上松弛变量的惩罚项，表示希望违反这项规则的样本点越少越好。
6.非线性支持向量机

## 支持向量机的推导
https://www.cnblogs.com/vipyoumay/p/7560061.html  

KKT条件参考（没仔细看，感觉就看看长啥样就行）：https://www.cnblogs.com/liaohuiqiang/p/7805954.html

## 支持向量机常见问题
https://www.jianshu.com/p/fa02098bc220  

## 为什么SVM对小样本数据集的预测效果好
为什么svm对小样本拟合效果好：在基本的机器学习算法中，svm的一个很大的优势是它的泛化能力，这种优势来源于svm中margin的概念本身就是用于最大化泛化能力（这是因为其本身的优化目标是结构化风险最小，而不是经验风险最小，因此，通过margin的概念，得到对数据分布分结构化描述，因此降低了对数据规模和数据分布的要求），利用非常小的训练样本就可以得到class的模式，而这种泛化能力是随着维数的增加而降低的，直观上的理解就是维数越高，越难得到class的模式（详细的介绍在Vapnic的统计学习理论）。在津南数字制造算法挑战赛中，样本量只有两千左右，是一个小样本问题，在同等条件下用svm和lightgbm进行建模，lightgbm的精度要高很多，所以武断一些下结论的话，在当前，svm的这种优势已经是可替代的了。但同时也发现，svm对离群点（指缺乏相应条件下的样本，比较极端的情况，并不是传感器故障造成的错误的数据）预测效果要完胜其它算法。  
参考：https://www.cnblogs.com/zhizhan/p/4430253.html  
http://www.aiseminar.cn/bbs/forum.php?mod=viewthread&tid=830  

